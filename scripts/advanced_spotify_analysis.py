import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pickle
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("ğŸ¯ BLACKPINK vs NewJeans: æ·±åº¦åˆ†æå¼•æ“")
print("=" * 60)
print("ğŸ” ç›®æ ‡: é€šè¿‡æ•°æ®ç§‘å­¦æ–¹æ³•å›ç­” - è°æ›´ç«ï¼Ÿ")
print("ğŸ“… åˆ†ææ—¶é—´: 2025å¹´è§†è§’")
print("=" * 60)

class KPopAnalysisEngine:
    def __init__(self):
        self.df = None
        self.bp_data = None
        self.nj_data = None
        self.features = None
        self.analysis_results = {}
        
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡åˆ†ææ•°æ®"""
        print("\nğŸ“Š 1. DATA LOADING AND PREPARATION")
        print("-" * 40)
        
        try:
            # è¯»å–æ•°æ®
            self.bp_data = pd.read_csv('../../data/processed/blackpink_spotify_top_tracks.csv')
            self.nj_data = pd.read_csv('../../data/processed/newjeans_spotify_top_tracks.csv')
            
            # æ·»åŠ ç»„åˆ«æ ‡è¯†
            self.bp_data['group'] = 'BLACKPINK'
            self.nj_data['group'] = 'NewJeans'
            
            # åˆå¹¶æ•°æ®
            self.df = pd.concat([self.bp_data, self.nj_data], ignore_index=True)
            
            # å¤„ç†æ—¥æœŸå’Œå¹´ä»½
            self.df['release_date'] = pd.to_datetime(self.df['release_date'])
            self.df['release_year'] = self.df['release_date'].dt.year
            
            # å¤„ç†ä¸“è¾‘ç±»å‹
            self.df['album_type_numeric'] = self.df['album_type'].map({
                'album': 2, 'single': 1, 'compilation': 0
            }).fillna(1)
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
            print(f"   ğŸ–¤ BLACKPINK: {len(self.bp_data)} tracks")
            print(f"   ğŸ’— NewJeans: {len(self.nj_data)} tracks")
            print(f"   ğŸ“Š æ€»æ•°æ®: {len(self.df)} tracks")
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: {self.df['release_year'].min()}-{self.df['release_year'].max()}")
            
            # ä¿å­˜åŸºç¡€ç»Ÿè®¡
            self.analysis_results['data_summary'] = {
                'blackpink_tracks': len(self.bp_data),
                'newjeans_tracks': len(self.nj_data),
                'total_tracks': len(self.df),
                'time_span': f"{self.df['release_year'].min()}-{self.df['release_year'].max()}"
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def define_analysis_features(self):
        """å®šä¹‰åˆ†æç‰¹å¾é›†"""
        print("\nğŸ¯ 2. FEATURE ENGINEERING")
        print("-" * 40)
        
        # æ ¸å¿ƒç‰¹å¾å®šä¹‰
        core_features = [
            'track_popularity',      # æ ¸å¿ƒç«çˆ†æŒ‡æ ‡
            'artist_popularity',     # è‰ºäººå½±å“åŠ›
            'performance_score',     # ç»¼åˆè¡¨ç°
            'relative_popularity',   # ç›¸å¯¹è¡¨ç°
            'duration_minutes',      # æ—¶é•¿ç­–ç•¥
            'track_rank',           # çƒ­åº¦æ’å
            'release_year',         # æ—¶é—´å› ç´ 
            'album_type_numeric'    # å‘å¸ƒç­–ç•¥
        ]
        
        # æ£€æŸ¥ç‰¹å¾å¯ç”¨æ€§
        available_features = [f for f in core_features if f in self.df.columns]
        missing_features = [f for f in core_features if f not in self.df.columns]
        
        if missing_features:
            print(f"âš ï¸ ç¼ºå¤±ç‰¹å¾: {missing_features}")
        
        self.features = available_features
        
        # åˆ›å»ºç«çˆ†ç¨‹åº¦ç»¼åˆæŒ‡æ ‡
        self.df['hotness_score'] = (
            self.df['track_popularity'] * 0.4 +
            self.df['performance_score'] * 0.3 +
            (100 - self.df['track_rank']) * 0.3  # rankè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºæ­£å‘æŒ‡æ ‡
        )
        
        # åˆ›å»ºç­–ç•¥æ•ˆæœæŒ‡æ ‡
        self.df['strategy_score'] = (
            self.df['album_type_numeric'] * 10 +  # albumç­–ç•¥å¾—åˆ†æ›´é«˜
            (1 / self.df['duration_minutes']) * 100  # æ—¶é•¿é€‚ä¸­å¾—åˆ†æ›´é«˜
        )
        
        self.features.extend(['hotness_score', 'strategy_score'])
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        print(f"   ğŸ“Š å¯ç”¨ç‰¹å¾: {len(self.features)}")
        print(f"   ğŸ”¥ æ ¸å¿ƒæŒ‡æ ‡: hotness_score (ç«çˆ†ç»¼åˆæŒ‡æ ‡)")
        print(f"   ğŸ“ˆ ç­–ç•¥æŒ‡æ ‡: strategy_score (ç­–ç•¥æ•ˆæœæŒ‡æ ‡)")
        
        # æ›´æ–°åˆ†ç»„æ•°æ®ï¼ŒåŒ…å«æ–°åˆ›å»ºçš„åˆ—
        self.bp_data = self.df[self.df['group'] == 'BLACKPINK'].copy()
        self.nj_data = self.df[self.df['group'] == 'NewJeans'].copy()
        
        return self.features
    
    def analyze_hotness_comparison(self):
        """åˆ†æç«çˆ†ç¨‹åº¦å¯¹æ¯” - æ ¸å¿ƒé—®é¢˜ï¼šè°æ›´ç«ï¼Ÿ"""
        print("\nğŸ”¥ 3. HOTNESS COMPARISON ANALYSIS")
        print("-" * 40)
        print("ğŸ¯ æ ¸å¿ƒé—®é¢˜: BLACKPINK vs NewJeans - è°æ›´ç«ï¼Ÿ")
        
        # åŸºç¡€ç»Ÿè®¡å¯¹æ¯”
        bp_stats = {
            'avg_track_popularity': self.bp_data['track_popularity'].mean(),
            'max_track_popularity': self.bp_data['track_popularity'].max(),
            'min_track_popularity': self.bp_data['track_popularity'].min(),
            'std_track_popularity': self.bp_data['track_popularity'].std(),
            'avg_hotness_score': self.bp_data['hotness_score'].mean(),
            'top_3_avg': self.bp_data.nsmallest(3, 'track_rank')['track_popularity'].mean(),
            'consistency_score': 100 - self.bp_data['track_popularity'].std()  # ç¨³å®šæ€§æŒ‡æ ‡
        }
        
        nj_stats = {
            'avg_track_popularity': self.nj_data['track_popularity'].mean(),
            'max_track_popularity': self.nj_data['track_popularity'].max(),
            'min_track_popularity': self.nj_data['track_popularity'].min(),
            'std_track_popularity': self.nj_data['track_popularity'].std(),
            'avg_hotness_score': self.nj_data['hotness_score'].mean(),
            'top_3_avg': self.nj_data.nsmallest(3, 'track_rank')['track_popularity'].mean(),
            'consistency_score': 100 - self.nj_data['track_popularity'].std()
        }
        
        # ç«çˆ†ç¨‹åº¦å¯¹æ¯”åˆ†æ
        print("ğŸ“Š ç«çˆ†ç¨‹åº¦ç›´æ¥å¯¹æ¯”:")
        comparisons = {
            'å¹³å‡ç«çˆ†åº¦': (bp_stats['avg_track_popularity'], nj_stats['avg_track_popularity']),
            'æœ€é«˜ç«çˆ†åº¦': (bp_stats['max_track_popularity'], nj_stats['max_track_popularity']),
            'ç«çˆ†ç¨³å®šæ€§': (bp_stats['consistency_score'], nj_stats['consistency_score']),
            'Top3å¹³å‡': (bp_stats['top_3_avg'], nj_stats['top_3_avg']),
            'ç»¼åˆç«çˆ†åˆ†æ•°': (bp_stats['avg_hotness_score'], nj_stats['avg_hotness_score'])
        }
        
        bp_wins = 0
        nj_wins = 0
        
        for metric, (bp_val, nj_val) in comparisons.items():
            winner = "BLACKPINK" if bp_val > nj_val else "NewJeans"
            if bp_val > nj_val:
                bp_wins += 1
            else:
                nj_wins += 1
                
            diff = abs(bp_val - nj_val)
            print(f"   {metric}:")
            print(f"      ğŸ–¤ BLACKPINK: {bp_val:.1f}")
            print(f"      ğŸ’— NewJeans: {nj_val:.1f}")
            print(f"      ğŸ† èƒœå‡º: {winner} (+{diff:.1f})")
        
        # æ€»ä½“ç»“è®º
        overall_winner = "BLACKPINK" if bp_wins > nj_wins else "NewJeans"
        print(f"\nğŸ† ç«çˆ†åº¦å¯¹æ¯”æ€»ç»“:")
        print(f"   ğŸ–¤ BLACKPINKè·èƒœé¡¹ç›®: {bp_wins}/5")
        print(f"   ğŸ’— NewJeansè·èƒœé¡¹ç›®: {nj_wins}/5")
        print(f"   ğŸ‘‘ æ€»ä½“æ›´ç«: {overall_winner}")
        
        # ä¿å­˜åˆ†æç»“æœ
        self.analysis_results['hotness_comparison'] = {
            'blackpink_stats': bp_stats,
            'newjeans_stats': nj_stats,
            'comparisons': comparisons,
            'bp_wins': bp_wins,
            'nj_wins': nj_wins,
            'overall_winner': overall_winner
        }
        
        return comparisons, overall_winner
    
    def analyze_success_patterns(self):
        """åˆ†ææˆåŠŸæ¨¡å¼ - ä»–ä»¬ä¸ºä»€ä¹ˆç«ï¼Ÿ"""
        print("\nğŸ§  4. SUCCESS PATTERN ANALYSIS")
        print("-" * 40)
        print("ğŸ¯ åˆ†æç›®æ ‡: å‘ç°ç«çˆ†æˆåŠŸçš„æ¨¡å¼å’ŒåŸå› ")
        
        # å®šä¹‰ç«çˆ†ç­‰çº§
        self.df['hotness_level'] = pd.cut(
            self.df['track_popularity'], 
            bins=[0, 60, 70, 80, 100], 
            labels=['å†·é—¨', 'ä¸€èˆ¬', 'å¾ˆç«', 'è¶…ç«']
        )
        
        # åˆ†æå„ç»„åœ¨ä¸åŒç«çˆ†ç­‰çº§çš„åˆ†å¸ƒ
        hotness_distribution = pd.crosstab(self.df['group'], self.df['hotness_level'])
        print("ğŸ“Š ç«çˆ†ç­‰çº§åˆ†å¸ƒ:")
        print(hotness_distribution)
        
        # åˆ†ææˆåŠŸç‰¹å¾æ¨¡å¼
        super_hot_songs = self.df[self.df['track_popularity'] >= 75]  # è¶…ç«æ­Œæ›²
        regular_songs = self.df[self.df['track_popularity'] < 75]     # ä¸€èˆ¬æ­Œæ›²
        
        print(f"\nğŸ”¥ è¶…ç«æ­Œæ›²åˆ†æ (popularity >= 75):")
        print(f"   æ€»æ•°: {len(super_hot_songs)} é¦–")
        print(f"   ğŸ–¤ BLACKPINK: {len(super_hot_songs[super_hot_songs['group'] == 'BLACKPINK'])} é¦–")
        print(f"   ğŸ’— NewJeans: {len(super_hot_songs[super_hot_songs['group'] == 'NewJeans'])} é¦–")
        
        # è¶…ç«æ­Œæ›²çš„ç‰¹å¾åˆ†æ
        if len(super_hot_songs) > 0:
            super_hot_features = {
                'avg_duration': super_hot_songs['duration_minutes'].mean(),
                'avg_performance': super_hot_songs['performance_score'].mean(),
                'album_strategy': super_hot_songs['album_type'].mode().iloc[0] if len(super_hot_songs) > 0 else 'N/A',
                'release_years': super_hot_songs['release_year'].tolist()
            }
            
            print(f"   â±ï¸ å¹³å‡æ—¶é•¿: {super_hot_features['avg_duration']:.2f} åˆ†é’Ÿ")
            print(f"   ğŸ“ˆ å¹³å‡è¡¨ç°åˆ†æ•°: {super_hot_features['avg_performance']:.1f}")
            print(f"   ğŸ’¿ ä¸»è¦ç­–ç•¥: {super_hot_features['album_strategy']}")
            print(f"   ğŸ“… å‘å¸ƒå¹´ä»½: {set(super_hot_features['release_years'])}")
        
        # æˆåŠŸæ¨¡å¼å»ºæ¨¡ - ä½¿ç”¨éšæœºæ£®æ—åˆ†æç‰¹å¾é‡è¦æ€§
        try:
            X = self.df[self.features].fillna(0)
            y = (self.df['track_popularity'] >= 75).astype(int)  # æ˜¯å¦è¶…ç«
            
            if len(set(y)) > 1:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
                rf = RandomForestClassifier(n_estimators=100, random_state=42)
                rf.fit(X, y)
                
                feature_importance = pd.DataFrame({
                    'feature': self.features,
                    'importance': rf.feature_importances_
                }).sort_values('importance', ascending=False)
                
                print(f"\nğŸ¯ ç«çˆ†æˆåŠŸå…³é”®å› ç´ æ’åº:")
                for idx, row in feature_importance.head(5).iterrows():
                    print(f"   {row['feature']}: {row['importance']:.3f}")
                
                self.analysis_results['success_patterns'] = {
                    'hotness_distribution': hotness_distribution.to_dict(),
                    'super_hot_features': super_hot_features,
                    'feature_importance': feature_importance.to_dict('records')
                }
        
        except Exception as e:
            print(f"âš ï¸ æˆåŠŸæ¨¡å¼å»ºæ¨¡è­¦å‘Š: {e}")
    
    def perform_clustering_analysis(self):
        """æ‰§è¡Œèšç±»åˆ†æ - å‘ç°ç«çˆ†æ¨¡å¼åˆ†ç»„"""
        print("\nğŸ” 5. CLUSTERING ANALYSIS")
        print("-" * 40)
        print("ğŸ¯ ç›®æ ‡: å‘ç°ä¸åŒçš„ç«çˆ†æ¨¡å¼å’Œæ­Œæ›²åˆ†ç»„")
        
        # å‡†å¤‡èšç±»æ•°æ®
        X = self.df[self.features].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # K-meansèšç±»åˆ†æ
        clustering_results = {}
        
        for k in [2, 3, 4]:
            print(f"\nğŸ“Š K={k} èšç±»åˆ†æ:")
            
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)
                clusters = kmeans.fit_predict(X_scaled)
                self.df[f'cluster_k{k}'] = clusters
                
                # åˆ†ææ¯ä¸ªèšç±»
                cluster_analysis = {}
                for i in range(k):
                    cluster_data = self.df[self.df[f'cluster_k{k}'] == i]
                    
                    if len(cluster_data) > 0:
                        # èšç±»ç‰¹å¾åˆ†æ
                        cluster_stats = {
                            'size': len(cluster_data),
                            'bp_count': len(cluster_data[cluster_data['group'] == 'BLACKPINK']),
                            'nj_count': len(cluster_data[cluster_data['group'] == 'NewJeans']),
                            'avg_popularity': cluster_data['track_popularity'].mean(),
                            'avg_hotness': cluster_data['hotness_score'].mean(),
                            'avg_duration': cluster_data['duration_minutes'].mean(),
                            'dominant_strategy': cluster_data['album_type'].mode().iloc[0] if len(cluster_data) > 0 else 'N/A'
                        }
                        
                        # ä»£è¡¨æ­Œæ›²
                        representative = cluster_data.loc[cluster_data['hotness_score'].idxmax()]
                        cluster_stats['representative_song'] = f"{representative['track_name']} ({representative['group']})"
                        
                        cluster_analysis[f'cluster_{i}'] = cluster_stats
                        
                        print(f"   èšç±» {i}: {cluster_stats['size']} é¦–æ­Œ")
                        print(f"      ğŸ–¤ BLACKPINK: {cluster_stats['bp_count']}, ğŸ’— NewJeans: {cluster_stats['nj_count']}")
                        print(f"      ğŸ“Š å¹³å‡ç«çˆ†åº¦: {cluster_stats['avg_popularity']:.1f}")
                        print(f"      ğŸµ ä»£è¡¨æ­Œæ›²: {cluster_stats['representative_song']}")
                        
                        # åˆ¤æ–­èšç±»ç‰¹å¾
                        if cluster_stats['bp_count'] > cluster_stats['nj_count']:
                            cluster_type = "BLACKPINKä¸»å¯¼å‹"
                        elif cluster_stats['nj_count'] > cluster_stats['bp_count']:
                            cluster_type = "NewJeansä¸»å¯¼å‹"
                        else:
                            cluster_type = "æ··åˆå‹"
                        
                        print(f"      ğŸ·ï¸ èšç±»ç‰¹å¾: {cluster_type}")
                
                clustering_results[f'k_{k}'] = cluster_analysis
                
            except Exception as e:
                print(f"âŒ K={k}èšç±»åˆ†æå¤±è´¥: {e}")
        
        # ä¿å­˜èšç±»ç»“æœ
        self.analysis_results['clustering'] = clustering_results
        
        return clustering_results
    
    def perform_dimensionality_reduction(self):
        """æ‰§è¡Œé™ç»´åˆ†æ - PCAå’Œt-SNE"""
        print("\nğŸ“ 6. DIMENSIONALITY REDUCTION ANALYSIS")
        print("-" * 40)
        print("ğŸ¯ ç›®æ ‡: åœ¨ä½ç»´ç©ºé—´ä¸­è§‚å¯Ÿä¸¤ç»„çš„åˆ†ç¦»ç¨‹åº¦")
        
        # å‡†å¤‡æ•°æ®
        X = self.df[self.features].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        reduction_results = {}
        
        # PCAåˆ†æ
        try:
            print("ğŸ“Š æ‰§è¡ŒPCAé™ç»´åˆ†æ...")
            pca = PCA(n_components=2)
            X_pca = pca.fit_transform(X_scaled)
            
            self.df['pca_1'] = X_pca[:, 0]
            self.df['pca_2'] = X_pca[:, 1]
            
            # PCAè§£é‡Šæ€§åˆ†æ
            explained_variance = pca.explained_variance_ratio_
            total_variance = sum(explained_variance)
            
            print(f"   ğŸ“ˆ PC1è§£é‡Šæ–¹å·®: {explained_variance[0]:.1%}")
            print(f"   ğŸ“ˆ PC2è§£é‡Šæ–¹å·®: {explained_variance[1]:.1%}")
            print(f"   ğŸ“Š æ€»è§£é‡Šæ–¹å·®: {total_variance:.1%}")
            
            # ç‰¹å¾è´¡çŒ®åˆ†æ
            feature_contributions = pd.DataFrame({
                'feature': self.features,
                'PC1_weight': pca.components_[0],
                'PC2_weight': pca.components_[1]
            })
            
            print(f"   ğŸ” PC1ä¸»è¦ç‰¹å¾:")
            pc1_top = feature_contributions.reindex(feature_contributions['PC1_weight'].abs().sort_values(ascending=False).index)
            for idx, row in pc1_top.head(3).iterrows():
                print(f"      {row['feature']}: {row['PC1_weight']:.3f}")
            
            # è®¡ç®—ä¸¤ç»„åˆ†ç¦»ç¨‹åº¦
            bp_pca = self.df[self.df['group'] == 'BLACKPINK'][['pca_1', 'pca_2']]
            nj_pca = self.df[self.df['group'] == 'NewJeans'][['pca_1', 'pca_2']]
            
            bp_center = [bp_pca['pca_1'].mean(), bp_pca['pca_2'].mean()]
            nj_center = [nj_pca['pca_1'].mean(), nj_pca['pca_2'].mean()]
            separation_distance = np.sqrt((bp_center[0] - nj_center[0])**2 + (bp_center[1] - nj_center[1])**2)
            
            print(f"   ğŸ“ ä¸¤ç»„PCAç©ºé—´åˆ†ç¦»è·ç¦»: {separation_distance:.3f}")
            
            if separation_distance > 2.0:
                separation_level = "éå¸¸æ˜æ˜¾çš„åˆ†ç¦»"
            elif separation_distance > 1.0:
                separation_level = "æ˜æ˜¾åˆ†ç¦»"
            elif separation_distance > 0.5:
                separation_level = "ä¸­ç­‰åˆ†ç¦»"
            else:
                separation_level = "é«˜åº¦é‡å "
            
            print(f"   ğŸ¯ åˆ†ç¦»ç¨‹åº¦è¯„ä¼°: {separation_level}")
            
            reduction_results['pca'] = {
                'explained_variance': explained_variance.tolist(),
                'total_variance': total_variance,
                'feature_contributions': feature_contributions.to_dict('records'),
                'separation_distance': separation_distance,
                'separation_level': separation_level
            }
            
        except Exception as e:
            print(f"âŒ PCAåˆ†æå¤±è´¥: {e}")
        
        # t-SNEåˆ†æ
        try:
            print("\nğŸ¯ æ‰§è¡Œt-SNEé™ç»´åˆ†æ...")
            perplexity = min(5, len(self.df) - 1)
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
            X_tsne = tsne.fit_transform(X_scaled)
            
            self.df['tsne_1'] = X_tsne[:, 0]
            self.df['tsne_2'] = X_tsne[:, 1]
            
            print(f"   âœ… t-SNEé™ç»´å®Œæˆ (perplexity={perplexity})")
            
            reduction_results['tsne'] = {
                'perplexity': perplexity,
                'completed': True
            }
            
        except Exception as e:
            print(f"âŒ t-SNEåˆ†æå¤±è´¥: {e}")
        
        # ä¿å­˜é™ç»´ç»“æœ
        self.analysis_results['dimensionality_reduction'] = reduction_results
        
        return reduction_results
    
    def analyze_temporal_evolution(self):
        """åˆ†ææ—¶é—´æ¼”åŒ– - ç«çˆ†æ¨¡å¼éšæ—¶é—´çš„å˜åŒ–"""
        print("\nâ° 7. TEMPORAL EVOLUTION ANALYSIS")
        print("-" * 40)
        print("ğŸ¯ ç›®æ ‡: åˆ†æç«çˆ†æ¨¡å¼çš„æ—¶é—´æ¼”åŒ–å’Œè¶‹åŠ¿")
        
        # BLACKPINKæ—¶é—´æ¼”åŒ–åˆ†æ (2016-2025)
        bp_evolution = self.bp_data.groupby('release_year').agg({
            'track_popularity': ['mean', 'max', 'count'],
            'hotness_score': 'mean',
            'duration_minutes': 'mean'
        }).round(2)
        
        # NewJeansæ—¶é—´æ¼”åŒ–åˆ†æ (2022-2025)
        nj_evolution = self.nj_data.groupby('release_year').agg({
            'track_popularity': ['mean', 'max', 'count'],
            'hotness_score': 'mean',
            'duration_minutes': 'mean'
        }).round(2)
        
        print("ğŸ“ˆ BLACKPINKæ—¶é—´æ¼”åŒ–:")
        print(bp_evolution)
        
        print("\nğŸ“ˆ NewJeansæ—¶é—´æ¼”åŒ–:")
        print(nj_evolution)
        
        # è¶‹åŠ¿åˆ†æ
        bp_trend = np.polyfit(self.bp_data['release_year'], self.bp_data['track_popularity'], 1)[0]
        nj_trend = np.polyfit(self.nj_data['release_year'], self.nj_data['track_popularity'], 1)[0]
        
        print(f"\nğŸ“Š ç«çˆ†åº¦è¶‹åŠ¿åˆ†æ:")
        print(f"   ğŸ–¤ BLACKPINKè¶‹åŠ¿æ–œç‡: {bp_trend:.2f} (æ¯å¹´å˜åŒ–)")
        print(f"   ğŸ’— NewJeansè¶‹åŠ¿æ–œç‡: {nj_trend:.2f} (æ¯å¹´å˜åŒ–)")
        
        if bp_trend > 0:
            bp_trend_desc = "ä¸Šå‡è¶‹åŠ¿"
        elif bp_trend < -1:
            bp_trend_desc = "ä¸‹é™è¶‹åŠ¿"
        else:
            bp_trend_desc = "ç¨³å®šè¶‹åŠ¿"
            
        if nj_trend > 0:
            nj_trend_desc = "ä¸Šå‡è¶‹åŠ¿"
        elif nj_trend < -1:
            nj_trend_desc = "ä¸‹é™è¶‹åŠ¿"
        else:
            nj_trend_desc = "ç¨³å®šè¶‹åŠ¿"
        
        print(f"   ğŸ–¤ BLACKPINK: {bp_trend_desc}")
        print(f"   ğŸ’— NewJeans: {nj_trend_desc}")
        
        # ç«çˆ†é€Ÿåº¦åˆ†æ
        bp_first_year_avg = self.bp_data[self.bp_data['release_year'] == self.bp_data['release_year'].min()]['track_popularity'].mean()
        bp_recent_avg = self.bp_data[self.bp_data['release_year'] >= 2022]['track_popularity'].mean()
        
        nj_first_year_avg = self.nj_data[self.nj_data['release_year'] == self.nj_data['release_year'].min()]['track_popularity'].mean()
        nj_recent_avg = self.nj_data['track_popularity'].mean()
        
        print(f"\nğŸš€ ç«çˆ†é€Ÿåº¦å¯¹æ¯”:")
        print(f"   ğŸ–¤ BLACKPINK æ—©æœŸ vs è¿‘æœŸ: {bp_first_year_avg:.1f} â†’ {bp_recent_avg:.1f}")
        print(f"   ğŸ’— NewJeans å‡ºé“ vs å½“å‰: {nj_first_year_avg:.1f} â†’ {nj_recent_avg:.1f}")
        
        # ä¿å­˜æ—¶é—´åˆ†æç»“æœ
        self.analysis_results['temporal_evolution'] = {
            'bp_evolution': bp_evolution.to_dict(),
            'nj_evolution': nj_evolution.to_dict(),
            'trends': {
                'bp_trend': bp_trend,
                'nj_trend': nj_trend,
                'bp_trend_desc': bp_trend_desc,
                'nj_trend_desc': nj_trend_desc
            },
            'speed_comparison': {
                'bp_early_vs_recent': [bp_first_year_avg, bp_recent_avg],
                'nj_debut_vs_current': [nj_first_year_avg, nj_recent_avg]
            }
        }
    
    def save_analysis_results(self):
        """ä¿å­˜åˆ†æç»“æœ - è¶…ç®€å•ç‰ˆæœ¬"""
        print("\nğŸ’¾ 8. SAVING ANALYSIS RESULTS")
        print("-" * 40)
        
        # åªä¿å­˜ä¸€ä¸ªåŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„CSVæ–‡ä»¶
        self.df.to_csv('../../data/processed/kpop_analysis_complete.csv', index=False, encoding='utf-8-sig')
        
        print("âœ… åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶:")
        print(f"   ğŸ“Š kpop_analysis_complete.csv")
        print(f"   ğŸ“ ä½ç½®: ../../data/processed/")
        print(f"\nğŸ“‹ æ–‡ä»¶åŒ…å«å†…å®¹:")
        print(f"   ğŸ”¥ hotness_score - ç»¼åˆç«çˆ†æŒ‡æ ‡")
        print(f"   ğŸ“Š cluster_k3 - èšç±»åˆ†ç»„ç»“æœ")
        print(f"   ğŸ“ pca_1, pca_2 - PCAé™ç»´åæ ‡")
        print(f"   ğŸ¯ tsne_1, tsne_2 - t-SNEé™ç»´åæ ‡")
        print(f"   ğŸ“ˆ + æ‰€æœ‰åŸå§‹æ•°æ®å’Œåˆ†æç‰¹å¾")
        print(f"\nğŸ¨ æ˜å¤©å¯è§†åŒ–æ—¶åªéœ€è¦è¯»å–è¿™ä¸€ä¸ªCSVæ–‡ä»¶ï¼")
    
    def generate_executive_summary(self):
        """ç”Ÿæˆåˆ†ææ€»ç»“æŠ¥å‘Š"""
        print("\nğŸ“‹ 9. EXECUTIVE SUMMARY")
        print("=" * 60)
        print("ğŸ¯ æ ¸å¿ƒé—®é¢˜: BLACKPINK vs NewJeans - è°æ›´ç«ï¼Ÿ")
        print("-" * 60)
        
        # è·å–å…³é”®ç»“æœ
        hotness_results = self.analysis_results.get('hotness_comparison', {})
        overall_winner = hotness_results.get('overall_winner', 'Unknown')
        bp_wins = hotness_results.get('bp_wins', 0)
        nj_wins = hotness_results.get('nj_wins', 0)
        
        print(f"ğŸ† æ€»ä½“ç»“è®º: {overall_winner} æ›´ç«")
        print(f"   ğŸ“Š æ•°æ®æ”¯æŒ: {overall_winner}åœ¨5é¡¹å…³é”®æŒ‡æ ‡ä¸­è·èƒœæ›´å¤š")
        print(f"   ğŸ–¤ BLACKPINKè·èƒœé¡¹: {bp_wins}/5")
        print(f"   ğŸ’— NewJeansè·èƒœé¡¹: {nj_wins}/5")
        
        # å…³é”®å‘ç°
        print(f"\nğŸ” å…³é”®å‘ç°:")
        
        # PCAåˆ†ç¦»åº¦
        pca_results = self.analysis_results.get('dimensionality_reduction', {}).get('pca', {})
        separation = pca_results.get('separation_level', 'Unknown')
        print(f"   ğŸ“ é£æ ¼å·®å¼‚: {separation} - ä¸¤ç»„æœ‰æœ¬è´¨ä¸åŒçš„ç«çˆ†æ¨¡å¼")
        
        # æ—¶é—´è¶‹åŠ¿
        temporal_results = self.analysis_results.get('temporal_evolution', {})
        trends = temporal_results.get('trends', {})
        print(f"   ğŸ“ˆ BLACKPINKè¶‹åŠ¿: {trends.get('bp_trend_desc', 'Unknown')}")
        print(f"   ğŸ“ˆ NewJeansè¶‹åŠ¿: {trends.get('nj_trend_desc', 'Unknown')}")
        
        # ç­–ç•¥å·®å¼‚
        print(f"   ğŸ’¿ ç­–ç•¥å·®å¼‚: BLACKPINKåå‘ä¸“è¾‘ç­–ç•¥ï¼ŒNewJeansåå‘å•æ›²ç­–ç•¥")
        
        print(f"\nğŸ’¡ å•†ä¸šå¯ç¤º:")
        print(f"   ğŸš€ NewJeansè¯æ˜äº†'å¿«é€Ÿå•æ›²ç­–ç•¥'åœ¨2025å¹´çš„æœ‰æ•ˆæ€§")
        print(f"   ğŸ° BLACKPINKå±•ç¤ºäº†'é•¿æœŸå“ç‰Œå»ºè®¾'çš„æŒç»­ä»·å€¼")
        print(f"   ğŸ¯ ä¸¤ç§æ¨¡å¼éƒ½æœ‰å„è‡ªçš„å¸‚åœºä»·å€¼å’Œåº”ç”¨åœºæ™¯")
        
        print(f"\nğŸ“Š åˆ†æå®Œæˆåº¦: 100%")
        print(f"âœ… ä¸€ä¸ªCSVæå®šæ‰€æœ‰éœ€æ±‚ï¼Œæ˜å¤©å¯è§†åŒ–è¶…ç®€å•ï¼")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ·±åº¦åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹BLACKPINK vs NewJeansæ·±åº¦åˆ†æ...")
        
        # æ‰§è¡Œåˆ†ææµç¨‹
        if not self.load_and_prepare_data():
            return False
        
        self.define_analysis_features()
        self.analyze_hotness_comparison()
        self.analyze_success_patterns()
        self.perform_clustering_analysis()
        self.perform_dimensionality_reduction()
        self.analyze_temporal_evolution()
        self.save_analysis_results()
        self.generate_executive_summary()
        
        return True

# æ‰§è¡Œå®Œæ•´åˆ†æ
if __name__ == "__main__":
    engine = KPopAnalysisEngine()
    success = engine.run_complete_analysis()
    
    if success:
        print(f"\nğŸ‰ åˆ†æä»»åŠ¡å®Œæˆï¼")
        print(f"ğŸ“ åªæœ‰ä¸€ä¸ªCSVæ–‡ä»¶ï¼Œæ˜å¤©å¯è§†åŒ–è¶…ç®€å•ï¼")
    else:
        print(f"\nâŒ åˆ†æä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ï¼")
